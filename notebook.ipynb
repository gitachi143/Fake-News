{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# NLP Data Pipeline for Fake News Detection\n",
    "\n",
    "This notebook implements a comprehensive NLP data pipeline for preprocessing and feature extraction on Kaggle articles and posts from the fake news detection dataset.\n",
    "\n",
    "## Pipeline Components:\n",
    "1. **Data Loading & Exploration** - Load and examine the dataset\n",
    "2. **Text Preprocessing** - Clean and normalize text data\n",
    "3. **Tokenization & Processing** - Advanced text processing with NLTK/spaCy\n",
    "4. **Feature Extraction** - TF-IDF, n-grams, word embeddings\n",
    "5. **Advanced Features** - Sentiment analysis, named entities, readability\n",
    "6. **Visualization** - Data exploration and analysis\n",
    "7. **Pipeline Class** - Reusable processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install kagglehub pandas numpy matplotlib seaborn nltk spacy textblob wordcloud scikit-learn plotly\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import required libraries\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abbab2",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddd27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"emineyetm/fake-news-detection-datasets\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List available files in the dataset\n",
    "dataset_files = os.listdir(path)\n",
    "print(\"\\nAvailable files in the dataset:\")\n",
    "for file in dataset_files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / (1024*1024)  # Size in MB\n",
    "        print(f\"  - {file} ({size:.2f} MB)\")\n",
    "\n",
    "# Load the main dataset files\n",
    "csv_files = [f for f in dataset_files if f.endswith('.csv')]\n",
    "print(f\"\\nLoading {len(csv_files)} CSV file(s)...\")\n",
    "\n",
    "dataframes = {}\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes[file] = df\n",
    "        print(f\"Loaded {file}: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"Dataset Exploration Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select the main dataframe (usually the largest one)\n",
    "if dataframes:\n",
    "    main_df_name = max(dataframes.keys(), key=lambda k: dataframes[k].shape[0])\n",
    "    df = dataframes[main_df_name]\n",
    "    print(f\"\\nWorking with main dataset: {main_df_name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\nColumn names and types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    print(\"\\nMissing values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Identify text columns\n",
    "    text_columns = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col.lower() in ['text', 'title', 'content', 'article', 'news', 'headline', 'body']:\n",
    "            text_columns.append(col)\n",
    "        elif df[col].dtype == 'object' and df[col].str.len().mean() > 20:  # Likely text if average length > 20 chars\n",
    "            text_columns.append(col)\n",
    "    \n",
    "    print(f\"\\nIdentified text columns: {text_columns}\")\n",
    "    \n",
    "    # Identify label column\n",
    "    label_columns = []\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['label', 'class', 'target', 'fake', 'real', 'category']:\n",
    "            label_columns.append(col)\n",
    "        elif df[col].nunique() <= 10 and df[col].dtype in ['int64', 'object']:  # Likely categorical\n",
    "            label_columns.append(col)\n",
    "    \n",
    "    print(f\"Identified potential label columns: {label_columns}\")\n",
    "    \n",
    "    # Show label distribution if available\n",
    "    if label_columns:\n",
    "        for col in label_columns[:2]:  # Show first 2 potential label columns\n",
    "            print(f\"\\nDistribution of '{col}':\")\n",
    "            print(df[col].value_counts())\n",
    "else:\n",
    "    print(\"No datasets loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27598f50",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase=True, \n",
    "                 remove_punctuation=True, \n",
    "                 remove_numbers=False,\n",
    "                 remove_stopwords=True, \n",
    "                 lemmatize=True,\n",
    "                 remove_urls=True,\n",
    "                 remove_emails=True,\n",
    "                 remove_html=True,\n",
    "                 min_length=2):\n",
    "        \n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_emails = remove_emails\n",
    "        self.remove_html = remove_html\n",
    "        self.min_length = min_length\n",
    "        \n",
    "        # Initialize NLTK components\n",
    "        if self.remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        if self.lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Apply all cleaning steps to a text string\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        if self.remove_html:\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            \n",
    "        # Remove URLs\n",
    "        if self.remove_urls:\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            \n",
    "        # Remove email addresses\n",
    "        if self.remove_emails:\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "            \n",
    "        # Remove extra whitespaces, newlines, tabs\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "        # Remove numbers\n",
    "        if self.remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "            \n",
    "        # Lemmatize\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            \n",
    "        # Remove short tokens\n",
    "        tokens = [token for token in tokens if len(token) >= self.min_length]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def process_dataframe(self, df, text_column, output_column=None):\n",
    "        \"\"\"Process a pandas DataFrame text column\"\"\"\n",
    "        if output_column is None:\n",
    "            output_column = f\"{text_column}_processed\"\n",
    "            \n",
    "        print(f\"Processing {len(df)} texts...\")\n",
    "        df[output_column] = df[text_column].apply(self.clean_text)\n",
    "        \n",
    "        # Calculate processing statistics\n",
    "        original_lengths = df[text_column].str.len().fillna(0)\n",
    "        processed_lengths = df[output_column].str.len().fillna(0)\n",
    "        \n",
    "        print(f\"Original text - Average length: {original_lengths.mean():.1f}, Max: {original_lengths.max()}\")\n",
    "        print(f\"Processed text - Average length: {processed_lengths.mean():.1f}, Max: {processed_lengths.max()}\")\n",
    "        print(f\"Length reduction: {((original_lengths.mean() - processed_lengths.mean()) / original_lengths.mean() * 100):.1f}%\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "print(\"Text Preprocessor initialized with default settings:\")\n",
    "print(f\"- Lowercase: {preprocessor.lowercase}\")\n",
    "print(f\"- Remove punctuation: {preprocessor.remove_punctuation}\")\n",
    "print(f\"- Remove stopwords: {preprocessor.remove_stopwords}\")\n",
    "print(f\"- Lemmatize: {preprocessor.lemmatize}\")\n",
    "print(f\"- Remove URLs: {preprocessor.remove_urls}\")\n",
    "print(f\"- Remove emails: {preprocessor.remove_emails}\")\n",
    "print(f\"- Remove HTML: {preprocessor.remove_html}\")\n",
    "print(f\"- Minimum token length: {preprocessor.min_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ee139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "if dataframes and text_columns:\n",
    "    # Use the first identified text column\n",
    "    main_text_column = text_columns[0]\n",
    "    print(f\"Preprocessing text column: '{main_text_column}'\")\n",
    "    \n",
    "    # Show before and after examples\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEFORE AND AFTER PREPROCESSING EXAMPLES:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_texts = df[main_text_column].dropna().head(3)\n",
    "    for i, (idx, original_text) in enumerate(sample_texts.items(), 1):\n",
    "        processed_text = preprocessor.clean_text(original_text)\n",
    "        \n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Original ({len(original_text)} chars):\")\n",
    "        print(f\"'{original_text[:200]}{'...' if len(original_text) > 200 else ''}'\")\n",
    "        print(f\"\\nProcessed ({len(processed_text)} chars):\")\n",
    "        print(f\"'{processed_text[:200]}{'...' if len(processed_text) > 200 else ''}'\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Apply preprocessing to the entire dataset\n",
    "    df = preprocessor.process_dataframe(df, main_text_column)\n",
    "    processed_column = f\"{main_text_column}_processed\"\n",
    "    \n",
    "    # Remove rows with empty processed text\n",
    "    original_rows = len(df)\n",
    "    df = df[df[processed_column].str.len() > 0]\n",
    "    print(f\"\\nRemoved {original_rows - len(df)} rows with empty processed text\")\n",
    "    print(f\"Final dataset size: {len(df)} rows\")\n",
    "    \n",
    "else:\n",
    "    print(\"No text columns identified for preprocessing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42137709",
   "metadata": {},
   "source": [
    "## 3. Advanced Tokenization and Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91339c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import textstat\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"spaCy model not found. Installing...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class AdvancedTextProcessor:\n",
    "    \"\"\"Advanced text processing using spaCy for NLP tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        \n",
    "    def extract_linguistic_features(self, text, max_length=1000000):\n",
    "        \"\"\"Extract advanced linguistic features from text\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str) or len(text) == 0:\n",
    "            return self._empty_features()\n",
    "        \n",
    "        # Truncate text if too long (spaCy has limits)\n",
    "        if len(text) > max_length:\n",
    "            text = text[:max_length]\n",
    "            \n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        features = {\n",
    "            # Token-level features\n",
    "            'token_count': len(doc),\n",
    "            'sentence_count': len(list(doc.sents)),\n",
    "            'avg_token_length': np.mean([len(token.text) for token in doc]) if doc else 0,\n",
    "            \n",
    "            # POS tag distribution\n",
    "            'noun_count': sum(1 for token in doc if token.pos_ == 'NOUN'),\n",
    "            'verb_count': sum(1 for token in doc if token.pos_ == 'VERB'),\n",
    "            'adj_count': sum(1 for token in doc if token.pos_ == 'ADJ'),\n",
    "            'adv_count': sum(1 for token in doc if token.pos_ == 'ADV'),\n",
    "            \n",
    "            # Named entities\n",
    "            'entity_count': len(doc.ents),\n",
    "            'person_count': sum(1 for ent in doc.ents if ent.label_ == 'PERSON'),\n",
    "            'org_count': sum(1 for ent in doc.ents if ent.label_ == 'ORG'),\n",
    "            'gpe_count': sum(1 for ent in doc.ents if ent.label_ == 'GPE'),  # Geopolitical entities\n",
    "            \n",
    "            # Readability metrics\n",
    "            'flesch_reading_ease': textstat.flesch_reading_ease(text),\n",
    "            'flesch_kincaid_grade': textstat.flesch_kincaid_grade(text),\n",
    "            'gunning_fog': textstat.gunning_fog(text),\n",
    "            \n",
    "            # Complexity metrics\n",
    "            'avg_sentence_length': np.mean([len(sent) for sent in doc.sents]) if list(doc.sents) else 0,\n",
    "            'unique_token_ratio': len(set(token.lemma_.lower() for token in doc if token.is_alpha)) / len(doc) if doc else 0,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _empty_features(self):\n",
    "        \"\"\"Return empty features for invalid text\"\"\"\n",
    "        return {\n",
    "            'token_count': 0, 'sentence_count': 0, 'avg_token_length': 0,\n",
    "            'noun_count': 0, 'verb_count': 0, 'adj_count': 0, 'adv_count': 0,\n",
    "            'entity_count': 0, 'person_count': 0, 'org_count': 0, 'gpe_count': 0,\n",
    "            'flesch_reading_ease': 0, 'flesch_kincaid_grade': 0, 'gunning_fog': 0,\n",
    "            'avg_sentence_length': 0, 'unique_token_ratio': 0\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        doc = self.nlp(text[:1000000])  # Limit text length\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        return entities\n",
    "    \n",
    "    def get_pos_tags(self, text):\n",
    "        \"\"\"Get part-of-speech tags for text\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        doc = self.nlp(text[:1000000])  # Limit text length\n",
    "        pos_tags = [(token.text, token.pos_, token.lemma_) for token in doc if token.is_alpha]\n",
    "        return pos_tags\n",
    "\n",
    "# Initialize advanced processor\n",
    "advanced_processor = AdvancedTextProcessor(nlp)\n",
    "print(\"Advanced text processor initialized with spaCy!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73891796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate advanced tokenization on sample texts\n",
    "if 'df' in locals() and not df.empty and text_columns:\n",
    "    print(\"Demonstrating Advanced Text Processing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Take a sample text for detailed analysis\n",
    "    sample_idx = df.index[0]\n",
    "    sample_text = df.loc[sample_idx, main_text_column]\n",
    "    \n",
    "    if pd.notna(sample_text) and len(sample_text) > 50:\n",
    "        print(f\"\\nSample Text Analysis:\")\n",
    "        print(f\"Text preview: {sample_text[:200]}...\")\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        features = advanced_processor.extract_linguistic_features(sample_text)\n",
    "        \n",
    "        print(f\"\\nLinguistic Features:\")\n",
    "        for feature, value in features.items():\n",
    "            print(f\"  {feature}: {value:.2f}\" if isinstance(value, float) else f\"  {feature}: {value}\")\n",
    "        \n",
    "        # Extract named entities\n",
    "        entities = advanced_processor.extract_entities(sample_text)\n",
    "        if entities:\n",
    "            print(f\"\\nNamed Entities (first 10):\")\n",
    "            for entity, label in entities[:10]:\n",
    "                print(f\"  {entity} ({label})\")\n",
    "        \n",
    "        # Show POS tags for first sentence\n",
    "        pos_tags = advanced_processor.get_pos_tags(sample_text)\n",
    "        if pos_tags:\n",
    "            print(f\"\\nPart-of-Speech Tags (first 15 tokens):\")\n",
    "            for token, pos, lemma in pos_tags[:15]:\n",
    "                print(f\"  {token} -> {pos} (lemma: {lemma})\")\n",
    "    \n",
    "    # Extract linguistic features for a subset of the data (for performance)\n",
    "    print(f\"\\nExtracting linguistic features for first 100 rows...\")\n",
    "    sample_df = df.head(100).copy()\n",
    "    \n",
    "    # Extract features\n",
    "    linguistic_features = []\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        text = row[main_text_column]\n",
    "        features = advanced_processor.extract_linguistic_features(text)\n",
    "        linguistic_features.append(features)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(linguistic_features)\n",
    "    \n",
    "    print(f\"\\nLinguistic Features Summary:\")\n",
    "    print(features_df.describe())\n",
    "    \n",
    "    # Store the features for later use\n",
    "    for col in features_df.columns:\n",
    "        sample_df[f'ling_{col}'] = features_df[col]\n",
    "    \n",
    "    print(f\"\\nAdded {len(features_df.columns)} linguistic features to the dataset!\")\n",
    "else:\n",
    "    print(\"No valid data available for advanced processing demonstration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8ddc6",
   "metadata": {},
   "source": [
    "## 4. Basic Feature Extraction (TF-IDF, N-grams, Word Counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a769c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract various text features for machine learning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tfidf_vectorizers = {}\n",
    "        self.count_vectorizers = {}\n",
    "        self.fitted = False\n",
    "        \n",
    "    def extract_basic_features(self, texts):\n",
    "        \"\"\"Extract basic statistical features from texts\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                text = \"\"\n",
    "            \n",
    "            # Basic counts\n",
    "            char_count = len(text)\n",
    "            word_count = len(text.split())\n",
    "            sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "            \n",
    "            # Average lengths\n",
    "            avg_word_length = np.mean([len(word) for word in text.split()]) if text.split() else 0\n",
    "            avg_sentence_length = word_count / max(sentence_count, 1)\n",
    "            \n",
    "            # Character statistics\n",
    "            uppercase_count = sum(1 for c in text if c.isupper())\n",
    "            digit_count = sum(1 for c in text if c.isdigit())\n",
    "            special_char_count = sum(1 for c in text if not c.isalnum() and not c.isspace())\n",
    "            \n",
    "            # Punctuation\n",
    "            punctuation_count = sum(1 for c in text if c in string.punctuation)\n",
    "            exclamation_count = text.count('!')\n",
    "            question_count = text.count('?')\n",
    "            \n",
    "            features.append({\n",
    "                'char_count': char_count,\n",
    "                'word_count': word_count,\n",
    "                'sentence_count': max(sentence_count, 1),  # At least 1 sentence\n",
    "                'avg_word_length': avg_word_length,\n",
    "                'avg_sentence_length': avg_sentence_length,\n",
    "                'uppercase_ratio': uppercase_count / max(char_count, 1),\n",
    "                'digit_ratio': digit_count / max(char_count, 1),\n",
    "                'special_char_ratio': special_char_count / max(char_count, 1),\n",
    "                'punctuation_ratio': punctuation_count / max(char_count, 1),\n",
    "                'exclamation_count': exclamation_count,\n",
    "                'question_count': question_count,\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def fit_tfidf(self, texts, ngram_range=(1, 1), max_features=5000, min_df=2, max_df=0.95):\n",
    "        \"\"\"Fit TF-IDF vectorizer\"\"\"\n",
    "        vectorizer_key = f\"tfidf_{ngram_range[0]}_{ngram_range[1]}\"\n",
    "        \n",
    "        self.tfidf_vectorizers[vectorizer_key] = TfidfVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            stop_words='english',\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        # Clean texts\n",
    "        clean_texts = [text if isinstance(text, str) else \"\" for text in texts]\n",
    "        \n",
    "        self.tfidf_vectorizers[vectorizer_key].fit(clean_texts)\n",
    "        self.fitted = True\n",
    "        \n",
    "        return vectorizer_key\n",
    "    \n",
    "    def transform_tfidf(self, texts, vectorizer_key):\n",
    "        \"\"\"Transform texts using fitted TF-IDF vectorizer\"\"\"\n",
    "        if vectorizer_key not in self.tfidf_vectorizers:\n",
    "            raise ValueError(f\"Vectorizer {vectorizer_key} not found. Fit first.\")\n",
    "        \n",
    "        # Clean texts\n",
    "        clean_texts = [text if isinstance(text, str) else \"\" for text in texts]\n",
    "        \n",
    "        tfidf_matrix = self.tfidf_vectorizers[vectorizer_key].transform(clean_texts)\n",
    "        feature_names = self.tfidf_vectorizers[vectorizer_key].get_feature_names_out()\n",
    "        \n",
    "        return tfidf_matrix, feature_names\n",
    "    \n",
    "    def get_top_tfidf_features(self, tfidf_matrix, feature_names, top_k=20):\n",
    "        \"\"\"Get top TF-IDF features across all documents\"\"\"\n",
    "        # Calculate mean TF-IDF scores\n",
    "        mean_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        \n",
    "        # Get top features\n",
    "        top_indices = mean_scores.argsort()[-top_k:][::-1]\n",
    "        top_features = [(feature_names[i], mean_scores[i]) for i in top_indices]\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def extract_ngrams(self, texts, n=2, top_k=50):\n",
    "        \"\"\"Extract top n-grams from texts\"\"\"\n",
    "        from nltk.util import ngrams\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        all_ngrams = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                continue\n",
    "            \n",
    "            tokens = word_tokenize(text.lower())\n",
    "            text_ngrams = list(ngrams(tokens, n))\n",
    "            all_ngrams.extend(text_ngrams)\n",
    "        \n",
    "        # Count n-grams\n",
    "        ngram_counts = Counter(all_ngrams)\n",
    "        top_ngrams = ngram_counts.most_common(top_k)\n",
    "        \n",
    "        return top_ngrams\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor()\n",
    "print(\"Feature extractor initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d424b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply basic feature extraction\n",
    "if 'df' in locals() and not df.empty and text_columns:\n",
    "    print(\"Extracting Basic Text Features\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Use processed text for feature extraction\n",
    "    if processed_column in df.columns:\n",
    "        text_data = df[processed_column].fillna(\"\").tolist()\n",
    "        original_text_data = df[main_text_column].fillna(\"\").tolist()\n",
    "    else:\n",
    "        text_data = df[main_text_column].fillna(\"\").tolist()\n",
    "        original_text_data = text_data.copy()\n",
    "    \n",
    "    # Extract basic statistical features from original text\n",
    "    print(\"Extracting basic statistical features...\")\n",
    "    basic_features_df = feature_extractor.extract_basic_features(original_text_data)\n",
    "    \n",
    "    print(f\"Basic Features Shape: {basic_features_df.shape}\")\n",
    "    print(\"\\nBasic Features Summary:\")\n",
    "    print(basic_features_df.describe())\n",
    "    \n",
    "    # Fit and transform TF-IDF features\n",
    "    print(f\"\\nFitting TF-IDF vectorizers...\")\n",
    "    \n",
    "    # Unigrams TF-IDF\n",
    "    unigram_key = feature_extractor.fit_tfidf(text_data, ngram_range=(1, 1), max_features=1000)\n",
    "    unigram_tfidf, unigram_features = feature_extractor.transform_tfidf(text_data, unigram_key)\n",
    "    \n",
    "    # Bigrams TF-IDF\n",
    "    bigram_key = feature_extractor.fit_tfidf(text_data, ngram_range=(2, 2), max_features=500)\n",
    "    bigram_tfidf, bigram_features = feature_extractor.transform_tfidf(text_data, bigram_key)\n",
    "    \n",
    "    # Trigrams TF-IDF  \n",
    "    trigram_key = feature_extractor.fit_tfidf(text_data, ngram_range=(3, 3), max_features=200)\n",
    "    trigram_tfidf, trigram_features = feature_extractor.transform_tfidf(text_data, trigram_key)\n",
    "    \n",
    "    print(f\"Unigram TF-IDF Shape: {unigram_tfidf.shape}\")\n",
    "    print(f\"Bigram TF-IDF Shape: {bigram_tfidf.shape}\")\n",
    "    print(f\"Trigram TF-IDF Shape: {trigram_tfidf.shape}\")\n",
    "    \n",
    "    # Get top features for each n-gram type\n",
    "    top_unigrams = feature_extractor.get_top_tfidf_features(unigram_tfidf, unigram_features, top_k=15)\n",
    "    top_bigrams = feature_extractor.get_top_tfidf_features(bigram_tfidf, bigram_features, top_k=15)\n",
    "    top_trigrams = feature_extractor.get_top_tfidf_features(trigram_tfidf, trigram_features, top_k=15)\n",
    "    \n",
    "    print(f\"\\nTop 15 Unigrams (by average TF-IDF score):\")\n",
    "    for feature, score in top_unigrams:\n",
    "        print(f\"  {feature}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop 15 Bigrams:\")\n",
    "    for feature, score in top_bigrams:\n",
    "        print(f\"  {feature}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop 15 Trigrams:\")\n",
    "    for feature, score in top_trigrams:\n",
    "        print(f\"  {feature}: {score:.4f}\")\n",
    "    \n",
    "    # Extract traditional n-grams using NLTK\n",
    "    print(f\"\\nExtracting traditional n-grams...\")\n",
    "    top_bigrams_nltk = feature_extractor.extract_ngrams(text_data, n=2, top_k=10)\n",
    "    top_trigrams_nltk = feature_extractor.extract_ngrams(text_data, n=3, top_k=10)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Frequent Bigrams:\")\n",
    "    for ngram, count in top_bigrams_nltk:\n",
    "        print(f\"  {' '.join(ngram)}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Frequent Trigrams:\")\n",
    "    for ngram, count in top_trigrams_nltk:\n",
    "        print(f\"  {' '.join(ngram)}: {count}\")\n",
    "    \n",
    "    # Store the extracted features for later use\n",
    "    feature_data = {\n",
    "        'basic_features': basic_features_df,\n",
    "        'unigram_tfidf': unigram_tfidf,\n",
    "        'bigram_tfidf': bigram_tfidf,\n",
    "        'trigram_tfidf': trigram_tfidf,\n",
    "        'unigram_features': unigram_features,\n",
    "        'bigram_features': bigram_features,\n",
    "        'trigram_features': trigram_features\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nFeature extraction completed! Stored features for {len(text_data)} documents.\")\n",
    "else:\n",
    "    print(\"No data available for feature extraction!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a3e8e",
   "metadata": {},
   "source": [
    "## 5. Advanced Features (Sentiment Analysis, Named Entities, Readability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57596872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data for sentiment analysis\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "class AdvancedFeatureExtractor:\n",
    "    \"\"\"Extract advanced NLP features for text analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    def extract_sentiment_features(self, texts):\n",
    "        \"\"\"Extract sentiment-related features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str) or len(text) == 0:\n",
    "                features.append({\n",
    "                    'textblob_polarity': 0.0,\n",
    "                    'textblob_subjectivity': 0.0,\n",
    "                    'vader_positive': 0.0,\n",
    "                    'vader_negative': 0.0,\n",
    "                    'vader_neutral': 0.0,\n",
    "                    'vader_compound': 0.0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # TextBlob sentiment\n",
    "            blob = TextBlob(text)\n",
    "            textblob_polarity = blob.sentiment.polarity\n",
    "            textblob_subjectivity = blob.sentiment.subjectivity\n",
    "            \n",
    "            # VADER sentiment\n",
    "            vader_scores = self.vader_analyzer.polarity_scores(text)\n",
    "            \n",
    "            features.append({\n",
    "                'textblob_polarity': textblob_polarity,\n",
    "                'textblob_subjectivity': textblob_subjectivity,\n",
    "                'vader_positive': vader_scores['pos'],\n",
    "                'vader_negative': vader_scores['neg'],\n",
    "                'vader_neutral': vader_scores['neu'],\n",
    "                'vader_compound': vader_scores['compound']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def extract_emotional_features(self, texts):\n",
    "        \"\"\"Extract emotion-related features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Define emotion words (simplified version)\n",
    "        emotion_words = {\n",
    "            'anger': ['angry', 'furious', 'rage', 'hate', 'mad', 'annoyed', 'irritated'],\n",
    "            'fear': ['afraid', 'scared', 'terrified', 'anxious', 'worried', 'nervous'],\n",
    "            'joy': ['happy', 'joyful', 'excited', 'delighted', 'pleased', 'glad'],\n",
    "            'sadness': ['sad', 'depressed', 'upset', 'disappointed', 'miserable'],\n",
    "            'surprise': ['surprised', 'amazed', 'shocked', 'astonished', 'stunned'],\n",
    "            'trust': ['trust', 'confident', 'reliable', 'believe', 'faith'],\n",
    "            'disgust': ['disgusting', 'repulsive', 'revolting', 'gross', 'awful']\n",
    "        }\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                text = \"\"\n",
    "            \n",
    "            text_lower = text.lower()\n",
    "            words = text_lower.split()\n",
    "            word_count = max(len(words), 1)  # Avoid division by zero\n",
    "            \n",
    "            emotion_features = {}\n",
    "            for emotion, emotion_word_list in emotion_words.items():\n",
    "                emotion_count = sum(1 for word in emotion_word_list if word in text_lower)\n",
    "                emotion_features[f'{emotion}_count'] = emotion_count\n",
    "                emotion_features[f'{emotion}_ratio'] = emotion_count / word_count\n",
    "            \n",
    "            features.append(emotion_features)\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def extract_complexity_features(self, texts):\n",
    "        \"\"\"Extract text complexity and readability features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str) or len(text) == 0:\n",
    "                features.append({\n",
    "                    'flesch_reading_ease': 0,\n",
    "                    'flesch_kincaid_grade': 0,\n",
    "                    'gunning_fog': 0,\n",
    "                    'coleman_liau_index': 0,\n",
    "                    'automated_readability_index': 0,\n",
    "                    'avg_syllables_per_word': 0,\n",
    "                    'difficult_words_ratio': 0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Various readability metrics\n",
    "            try:\n",
    "                flesch_ease = textstat.flesch_reading_ease(text)\n",
    "                flesch_grade = textstat.flesch_kincaid_grade(text)\n",
    "                gunning_fog = textstat.gunning_fog(text)\n",
    "                coleman_liau = textstat.coleman_liau_index(text)\n",
    "                auto_readability = textstat.automated_readability_index(text)\n",
    "                \n",
    "                # Syllable analysis\n",
    "                avg_syllables = textstat.avg_sentence_per_word(text)\n",
    "                difficult_words = textstat.difficult_words(text)\n",
    "                word_count = len(text.split())\n",
    "                difficult_ratio = difficult_words / max(word_count, 1)\n",
    "                \n",
    "            except:\n",
    "                # Fallback values if textstat fails\n",
    "                flesch_ease = flesch_grade = gunning_fog = 0\n",
    "                coleman_liau = auto_readability = avg_syllables = difficult_ratio = 0\n",
    "            \n",
    "            features.append({\n",
    "                'flesch_reading_ease': flesch_ease,\n",
    "                'flesch_kincaid_grade': flesch_grade,\n",
    "                'gunning_fog': gunning_fog,\n",
    "                'coleman_liau_index': coleman_liau,\n",
    "                'automated_readability_index': auto_readability,\n",
    "                'avg_syllables_per_word': avg_syllables,\n",
    "                'difficult_words_ratio': difficult_ratio\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def extract_style_features(self, texts):\n",
    "        \"\"\"Extract writing style features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                text = \"\"\n",
    "            \n",
    "            # Count different types of punctuation and style elements\n",
    "            exclamations = text.count('!')\n",
    "            questions = text.count('?')\n",
    "            quotes = text.count('\"') + text.count(\"'\")\n",
    "            ellipsis = text.count('...')\n",
    "            caps_words = sum(1 for word in text.split() if word.isupper() and len(word) > 1)\n",
    "            \n",
    "            # Sentence length variation\n",
    "            sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "            if sentences:\n",
    "                sentence_lengths = [len(s.split()) for s in sentences]\n",
    "                avg_sentence_length = np.mean(sentence_lengths)\n",
    "                sentence_length_std = np.std(sentence_lengths)\n",
    "            else:\n",
    "                avg_sentence_length = sentence_length_std = 0\n",
    "            \n",
    "            word_count = len(text.split())\n",
    "            \n",
    "            features.append({\n",
    "                'exclamation_density': exclamations / max(word_count, 1),\n",
    "                'question_density': questions / max(word_count, 1),\n",
    "                'quote_density': quotes / max(word_count, 1),\n",
    "                'ellipsis_count': ellipsis,\n",
    "                'caps_words_ratio': caps_words / max(word_count, 1),\n",
    "                'avg_sentence_length': avg_sentence_length,\n",
    "                'sentence_length_variation': sentence_length_std,\n",
    "                'total_sentences': len(sentences)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "\n",
    "# Initialize advanced feature extractor\n",
    "advanced_feature_extractor = AdvancedFeatureExtractor()\n",
    "print(\"Advanced feature extractor initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply advanced feature extraction\n",
    "if 'df' in locals() and not df.empty and 'text_data' in locals():\n",
    "    print(\"Extracting Advanced Text Features\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Use a subset for demonstration (first 100 rows for performance)\n",
    "    demo_texts = original_text_data[:100] if len(original_text_data) > 100 else original_text_data\n",
    "    \n",
    "    print(f\"Analyzing {len(demo_texts)} texts for advanced features...\")\n",
    "    \n",
    "    # Extract sentiment features\n",
    "    print(\"\\nExtracting sentiment features...\")\n",
    "    sentiment_features = advanced_feature_extractor.extract_sentiment_features(demo_texts)\n",
    "    print(f\"Sentiment Features Shape: {sentiment_features.shape}\")\n",
    "    print(\"Sentiment Features Summary:\")\n",
    "    print(sentiment_features.describe())\n",
    "    \n",
    "    # Extract emotional features\n",
    "    print(\"\\nExtracting emotional features...\")\n",
    "    emotional_features = advanced_feature_extractor.extract_emotional_features(demo_texts)\n",
    "    print(f\"Emotional Features Shape: {emotional_features.shape}\")\n",
    "    print(\"Top Emotional Features:\")\n",
    "    emotion_summary = emotional_features.mean().sort_values(ascending=False)\n",
    "    print(emotion_summary.head(10))\n",
    "    \n",
    "    # Extract complexity features\n",
    "    print(\"\\nExtracting complexity features...\")\n",
    "    complexity_features = advanced_feature_extractor.extract_complexity_features(demo_texts)\n",
    "    print(f\"Complexity Features Shape: {complexity_features.shape}\")\n",
    "    print(\"Complexity Features Summary:\")\n",
    "    print(complexity_features.describe())\n",
    "    \n",
    "    # Extract style features\n",
    "    print(\"\\nExtracting style features...\")\n",
    "    style_features = advanced_feature_extractor.extract_style_features(demo_texts)\n",
    "    print(f\"Style Features Shape: {style_features.shape}\")\n",
    "    print(\"Style Features Summary:\")\n",
    "    print(style_features.describe())\n",
    "    \n",
    "    # Combine all advanced features\n",
    "    advanced_features_combined = pd.concat([\n",
    "        sentiment_features,\n",
    "        emotional_features,\n",
    "        complexity_features,\n",
    "        style_features\n",
    "    ], axis=1)\n",
    "    \n",
    "    print(f\"\\nCombined Advanced Features Shape: {advanced_features_combined.shape}\")\n",
    "    print(f\"Total advanced features extracted: {advanced_features_combined.shape[1]}\")\n",
    "    \n",
    "    # Show some interesting insights\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ADVANCED FEATURES INSIGHTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Sentiment insights\n",
    "    avg_polarity = sentiment_features['textblob_polarity'].mean()\n",
    "    avg_subjectivity = sentiment_features['textblob_subjectivity'].mean()\n",
    "    \n",
    "    sentiment_label = \"Positive\" if avg_polarity > 0.1 else \"Negative\" if avg_polarity < -0.1 else \"Neutral\"\n",
    "    subjectivity_label = \"Subjective\" if avg_subjectivity > 0.5 else \"Objective\"\n",
    "    \n",
    "    print(f\"Average Sentiment: {sentiment_label} (polarity: {avg_polarity:.3f})\")\n",
    "    print(f\"Average Subjectivity: {subjectivity_label} ({avg_subjectivity:.3f})\")\n",
    "    \n",
    "    # Readability insights  \n",
    "    avg_flesch = complexity_features['flesch_reading_ease'].mean()\n",
    "    reading_level = (\"Very Easy\" if avg_flesch >= 90 else\n",
    "                    \"Easy\" if avg_flesch >= 80 else\n",
    "                    \"Fairly Easy\" if avg_flesch >= 70 else\n",
    "                    \"Standard\" if avg_flesch >= 60 else\n",
    "                    \"Fairly Difficult\" if avg_flesch >= 50 else\n",
    "                    \"Difficult\" if avg_flesch >= 30 else\n",
    "                    \"Very Difficult\")\n",
    "    \n",
    "    print(f\"Reading Level: {reading_level} (Flesch Score: {avg_flesch:.1f})\")\n",
    "    \n",
    "    # Emotional insights\n",
    "    top_emotions = emotional_features[[col for col in emotional_features.columns if col.endswith('_ratio')]].mean().sort_values(ascending=False)\n",
    "    if len(top_emotions) > 0:\n",
    "        dominant_emotion = top_emotions.index[0].replace('_ratio', '').capitalize()\n",
    "        emotion_score = top_emotions.iloc[0]\n",
    "        print(f\"Dominant Emotion: {dominant_emotion} (ratio: {emotion_score:.4f})\")\n",
    "    \n",
    "    # Style insights\n",
    "    avg_exclamation = style_features['exclamation_density'].mean()\n",
    "    avg_question = style_features['question_density'].mean()\n",
    "    \n",
    "    if avg_exclamation > 0.01:\n",
    "        print(f\"Writing Style: Emphatic (high exclamation usage: {avg_exclamation:.4f})\")\n",
    "    elif avg_question > 0.01:\n",
    "        print(f\"Writing Style: Inquisitive (high question usage: {avg_question:.4f})\")\n",
    "    else:\n",
    "        print(\"Writing Style: Formal/Neutral\")\n",
    "    \n",
    "    # Store advanced features for later use\n",
    "    advanced_feature_data = {\n",
    "        'sentiment_features': sentiment_features,\n",
    "        'emotional_features': emotional_features,\n",
    "        'complexity_features': complexity_features,\n",
    "        'style_features': style_features,\n",
    "        'combined_features': advanced_features_combined\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAdvanced feature extraction completed!\")\n",
    "    print(f\"Total features per document: {advanced_features_combined.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for advanced feature extraction!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a42f9",
   "metadata": {},
   "source": [
    "## 6. Word Embeddings and Semantic Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a76317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for embeddings\n",
    "!pip install gensim transformers torch sentence-transformers\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class EmbeddingExtractor:\n",
    "    \"\"\"Extract various types of word and sentence embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2vec_model = None\n",
    "        self.sentence_transformer = None\n",
    "        self.embedding_models = {}\n",
    "        \n",
    "    def load_word2vec_model(self, model_name='word2vec-google-news-300'):\n",
    "        \"\"\"Load pre-trained Word2Vec model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Word2Vec model: {model_name}\")\n",
    "            self.word2vec_model = api.load(model_name)\n",
    "            print(f\"Word2Vec model loaded! Vocabulary size: {len(self.word2vec_model.key_to_index)}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Word2Vec model: {e}\")\n",
    "            print(\"Continuing without Word2Vec features...\")\n",
    "            return False\n",
    "    \n",
    "    def load_sentence_transformer(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"Load sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading Sentence Transformer: {model_name}\")\n",
    "            self.sentence_transformer = SentenceTransformer(model_name)\n",
    "            print(\"Sentence Transformer loaded successfully!\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Sentence Transformer: {e}\")\n",
    "            print(\"Continuing without sentence transformer features...\")\n",
    "            return False\n",
    "    \n",
    "    def get_word2vec_features(self, texts, embedding_size=300):\n",
    "        \"\"\"Extract Word2Vec-based features from texts\"\"\"\n",
    "        if not self.word2vec_model:\n",
    "            print(\"Word2Vec model not loaded. Skipping Word2Vec features.\")\n",
    "            return None\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                # Return zero vector for invalid text\n",
    "                features.append(np.zeros(embedding_size))\n",
    "                continue\n",
    "            \n",
    "            words = text.lower().split()\n",
    "            word_vectors = []\n",
    "            \n",
    "            for word in words:\n",
    "                if word in self.word2vec_model.key_to_index:\n",
    "                    word_vectors.append(self.word2vec_model[word])\n",
    "            \n",
    "            if word_vectors:\n",
    "                # Average word vectors to get document vector\n",
    "                doc_vector = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                # No words found in vocabulary\n",
    "                doc_vector = np.zeros(embedding_size)\n",
    "            \n",
    "            features.append(doc_vector)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def get_sentence_transformer_features(self, texts):\n",
    "        \"\"\"Extract sentence transformer embeddings\"\"\"\n",
    "        if not self.sentence_transformer:\n",
    "            print(\"Sentence Transformer not loaded. Skipping sentence embeddings.\")\n",
    "            return None\n",
    "        \n",
    "        # Clean texts\n",
    "        clean_texts = [text if isinstance(text, str) else \"\" for text in texts]\n",
    "        \n",
    "        try:\n",
    "            embeddings = self.sentence_transformer.encode(clean_texts, show_progress_bar=True)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sentence embeddings: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_semantic_similarity_features(self, texts, reference_texts=None):\n",
    "        \"\"\"Extract semantic similarity features\"\"\"\n",
    "        if not self.sentence_transformer:\n",
    "            return None\n",
    "        \n",
    "        # Use first few texts as reference if not provided\n",
    "        if reference_texts is None:\n",
    "            reference_texts = texts[:min(10, len(texts))]\n",
    "        \n",
    "        # Get embeddings\n",
    "        text_embeddings = self.get_sentence_transformer_features(texts)\n",
    "        ref_embeddings = self.get_sentence_transformer_features(reference_texts)\n",
    "        \n",
    "        if text_embeddings is None or ref_embeddings is None:\n",
    "            return None\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(text_embeddings, ref_embeddings)\n",
    "        \n",
    "        # Extract features\n",
    "        features = {\n",
    "            'max_similarity': np.max(similarities, axis=1),\n",
    "            'mean_similarity': np.mean(similarities, axis=1),\n",
    "            'min_similarity': np.min(similarities, axis=1),\n",
    "            'similarity_std': np.std(similarities, axis=1)\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def reduce_embeddings_pca(self, embeddings, n_components=50):\n",
    "        \"\"\"Reduce embedding dimensionality using PCA\"\"\"\n",
    "        if embeddings is None:\n",
    "            return None\n",
    "        \n",
    "        pca = PCA(n_components=min(n_components, embeddings.shape[1]))\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        print(f\"PCA: Reduced from {embeddings.shape[1]} to {reduced_embeddings.shape[1]} dimensions\")\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "        \n",
    "        return reduced_embeddings, pca\n",
    "\n",
    "# Initialize embedding extractor\n",
    "embedding_extractor = EmbeddingExtractor()\n",
    "print(\"Embedding extractor initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e07c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply word embedding extraction\n",
    "if 'df' in locals() and not df.empty and 'text_data' in locals():\n",
    "    print(\"Extracting Word Embeddings and Semantic Features\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Use a smaller subset for embeddings (computationally expensive)\n",
    "    embedding_subset = original_text_data[:50] if len(original_text_data) > 50 else original_text_data\n",
    "    print(f\"Processing {len(embedding_subset)} texts for embeddings...\")\n",
    "    \n",
    "    # Try to load sentence transformer (more reliable than Word2Vec)\n",
    "    print(\"\\nLoading embedding models...\")\n",
    "    st_loaded = embedding_extractor.load_sentence_transformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    embedding_features = {}\n",
    "    \n",
    "    if st_loaded:\n",
    "        print(\"\\nExtracting sentence transformer embeddings...\")\n",
    "        sentence_embeddings = embedding_extractor.get_sentence_transformer_features(embedding_subset)\n",
    "        \n",
    "        if sentence_embeddings is not None:\n",
    "            print(f\"Sentence embeddings shape: {sentence_embeddings.shape}\")\n",
    "            \n",
    "            # Reduce dimensionality for visualization and efficiency\n",
    "            reduced_embeddings, pca = embedding_extractor.reduce_embeddings_pca(\n",
    "                sentence_embeddings, n_components=20\n",
    "            )\n",
    "            \n",
    "            print(f\"Reduced embeddings shape: {reduced_embeddings.shape}\")\n",
    "            \n",
    "            # Convert to DataFrame for easier handling\n",
    "            embedding_df = pd.DataFrame(\n",
    "                reduced_embeddings, \n",
    "                columns=[f'embedding_{i}' for i in range(reduced_embeddings.shape[1])]\n",
    "            )\n",
    "            \n",
    "            embedding_features['sentence_embeddings'] = embedding_df\n",
    "            \n",
    "            # Extract semantic similarity features\n",
    "            print(\"\\nExtracting semantic similarity features...\")\n",
    "            similarity_features = embedding_extractor.extract_semantic_similarity_features(\n",
    "                embedding_subset, reference_texts=embedding_subset[:5]\n",
    "            )\n",
    "            \n",
    "            if similarity_features is not None:\n",
    "                print(f\"Similarity features shape: {similarity_features.shape}\")\n",
    "                print(\"Similarity features summary:\")\n",
    "                print(similarity_features.describe())\n",
    "                embedding_features['similarity_features'] = similarity_features\n",
    "    \n",
    "    # Try to load Word2Vec (optional, may be slow/fail)\n",
    "    print(\"\\nAttempting to load Word2Vec model (this may take time or fail)...\")\n",
    "    try:\n",
    "        # Use a smaller Word2Vec model for faster loading\n",
    "        w2v_loaded = embedding_extractor.load_word2vec_model('glove-wiki-gigaword-50')\n",
    "        \n",
    "        if w2v_loaded:\n",
    "            print(\"Extracting Word2Vec features...\")\n",
    "            w2v_embeddings = embedding_extractor.get_word2vec_features(embedding_subset, embedding_size=50)\n",
    "            \n",
    "            if w2v_embeddings is not None:\n",
    "                print(f\"Word2Vec embeddings shape: {w2v_embeddings.shape}\")\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                w2v_df = pd.DataFrame(\n",
    "                    w2v_embeddings,\n",
    "                    columns=[f'w2v_{i}' for i in range(w2v_embeddings.shape[1])]\n",
    "                )\n",
    "                \n",
    "                embedding_features['word2vec_embeddings'] = w2v_df\n",
    "    except Exception as e:\n",
    "        print(f\"Word2Vec loading failed: {e}\")\n",
    "        print(\"Continuing without Word2Vec features...\")\n",
    "    \n",
    "    # Summary of extracted embedding features\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EMBEDDING FEATURES SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    total_embedding_features = 0\n",
    "    for feature_type, features in embedding_features.items():\n",
    "        feature_count = features.shape[1] if hasattr(features, 'shape') else 0\n",
    "        total_embedding_features += feature_count\n",
    "        print(f\"{feature_type}: {feature_count} features\")\n",
    "    \n",
    "    print(f\"\\nTotal embedding features: {total_embedding_features}\")\n",
    "    \n",
    "    if embedding_features:\n",
    "        print(\"\\nEmbedding extraction completed successfully!\")\n",
    "        \n",
    "        # Show some interesting analysis\n",
    "        if 'sentence_embeddings' in embedding_features:\n",
    "            print(\"\\nSample embedding analysis:\")\n",
    "            embeddings_sample = embedding_features['sentence_embeddings']\n",
    "            \n",
    "            # Calculate pairwise similarities for first few documents\n",
    "            sample_similarities = cosine_similarity(\n",
    "                sentence_embeddings[:5], \n",
    "                sentence_embeddings[:5]\n",
    "            )\n",
    "            \n",
    "            print(\"Pairwise similarities between first 5 documents:\")\n",
    "            for i in range(5):\n",
    "                for j in range(i+1, 5):\n",
    "                    sim_score = sample_similarities[i, j]\n",
    "                    print(f\"Doc {i} - Doc {j}: {sim_score:.3f}\")\n",
    "    else:\n",
    "        print(\"No embedding features were successfully extracted.\")\n",
    "        \n",
    "    # Store embedding features for later use\n",
    "    if embedding_features:\n",
    "        globals()['embedding_feature_data'] = embedding_features\n",
    "        \n",
    "else:\n",
    "    print(\"No data available for embedding extraction!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318578e7",
   "metadata": {},
   "source": [
    "## 7. Data Visualization and Exploratory Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27569571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional visualization packages\n",
    "!pip install wordcloud plotly\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DataVisualizer:\n",
    "    \"\"\"Create comprehensive visualizations for text data analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, figsize=(12, 8)):\n",
    "        self.figsize = figsize\n",
    "        plt.rcParams['figure.figsize'] = figsize\n",
    "    \n",
    "    def plot_text_length_distribution(self, texts, title=\"Text Length Distribution\"):\n",
    "        \"\"\"Plot distribution of text lengths\"\"\"\n",
    "        lengths = [len(str(text)) if pd.notna(text) else 0 for text in texts]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0, 0].hist(lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Text Length Histogram')\n",
    "        axes[0, 0].set_xlabel('Character Count')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Box plot\n",
    "        axes[0, 1].boxplot(lengths, vert=True)\n",
    "        axes[0, 1].set_title('Text Length Box Plot')\n",
    "        axes[0, 1].set_ylabel('Character Count')\n",
    "        \n",
    "        # Log scale histogram\n",
    "        axes[1, 0].hist(lengths, bins=50, alpha=0.7, color='lightcoral', edgecolor='black', log=True)\n",
    "        axes[1, 0].set_title('Text Length Histogram (Log Scale)')\n",
    "        axes[1, 0].set_xlabel('Character Count')\n",
    "        axes[1, 0].set_ylabel('Log Frequency')\n",
    "        \n",
    "        # Statistics text\n",
    "        stats_text = f\"\"\"Statistics:\n",
    "        Mean: {np.mean(lengths):.1f}\n",
    "        Median: {np.median(lengths):.1f}\n",
    "        Std: {np.std(lengths):.1f}\n",
    "        Min: {np.min(lengths)}\n",
    "        Max: {np.max(lengths)}\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12, transform=axes[1, 1].transAxes,\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
    "        axes[1, 1].set_title('Length Statistics')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_wordcloud(self, texts, title=\"Word Cloud\", max_words=200):\n",
    "        \"\"\"Create word cloud from texts\"\"\"\n",
    "        # Combine all texts\n",
    "        all_text = ' '.join([str(text) for text in texts if pd.notna(text)])\n",
    "        \n",
    "        if len(all_text.strip()) == 0:\n",
    "            print(\"No valid text data for word cloud\")\n",
    "            return\n",
    "        \n",
    "        # Create word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            max_words=max_words,\n",
    "            background_color='white',\n",
    "            colormap='viridis',\n",
    "            collocations=False\n",
    "        ).generate(all_text)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title, fontsize=18, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_correlation_matrix(self, features_df, title=\"Feature Correlation Matrix\"):\n",
    "        \"\"\"Plot correlation matrix of features\"\"\"\n",
    "        if features_df.empty:\n",
    "            print(\"No features available for correlation analysis\")\n",
    "            return\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = features_df.corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": .5}, fmt='.2f')\n",
    "        plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_distributions(self, features_df, title=\"Feature Distributions\", max_features=12):\n",
    "        \"\"\"Plot distributions of numerical features\"\"\"\n",
    "        if features_df.empty:\n",
    "            print(\"No features available for distribution analysis\")\n",
    "            return\n",
    "        \n",
    "        # Select numerical columns\n",
    "        numeric_cols = features_df.select_dtypes(include=[np.number]).columns[:max_features]\n",
    "        \n",
    "        n_features = len(numeric_cols)\n",
    "        n_cols = 4\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            row = i // n_cols\n",
    "            col_idx = i % n_cols\n",
    "            \n",
    "            if n_rows == 1:\n",
    "                ax = axes[col_idx] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col_idx]\n",
    "            \n",
    "            features_df[col].hist(bins=30, alpha=0.7, ax=ax, color='skyblue', edgecolor='black')\n",
    "            ax.set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel('Value')\n",
    "            ax.set_ylabel('Frequency')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_features, n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col_idx = i % n_cols\n",
    "            if n_rows == 1:\n",
    "                ax = axes[col_idx] if n_cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[row, col_idx]\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_sentiment_analysis(self, sentiment_features, title=\"Sentiment Analysis\"):\n",
    "        \"\"\"Plot sentiment analysis results\"\"\"\n",
    "        if sentiment_features is None or sentiment_features.empty:\n",
    "            print(\"No sentiment features available for plotting\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # TextBlob Polarity\n",
    "        axes[0, 0].hist(sentiment_features['textblob_polarity'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "        axes[0, 0].set_title('TextBlob Polarity')\n",
    "        axes[0, 0].set_xlabel('Polarity (-1: Negative, +1: Positive)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # TextBlob Subjectivity\n",
    "        axes[0, 1].hist(sentiment_features['textblob_subjectivity'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[0, 1].set_title('TextBlob Subjectivity')\n",
    "        axes[0, 1].set_xlabel('Subjectivity (0: Objective, 1: Subjective)')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # VADER Compound\n",
    "        axes[0, 2].hist(sentiment_features['vader_compound'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "        axes[0, 2].set_title('VADER Compound Score')\n",
    "        axes[0, 2].set_xlabel('Compound Score (-1: Negative, +1: Positive)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # VADER Components\n",
    "        vader_cols = ['vader_positive', 'vader_negative', 'vader_neutral']\n",
    "        vader_means = [sentiment_features[col].mean() for col in vader_cols]\n",
    "        \n",
    "        axes[1, 0].bar(['Positive', 'Negative', 'Neutral'], vader_means, \n",
    "                      color=['green', 'red', 'gray'], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_title('Average VADER Sentiment Components')\n",
    "        axes[1, 0].set_ylabel('Average Score')\n",
    "        \n",
    "        # Sentiment Scatter: Polarity vs Subjectivity\n",
    "        axes[1, 1].scatter(sentiment_features['textblob_polarity'], \n",
    "                          sentiment_features['textblob_subjectivity'], \n",
    "                          alpha=0.6, color='orange')\n",
    "        axes[1, 1].set_title('Polarity vs Subjectivity')\n",
    "        axes[1, 1].set_xlabel('Polarity')\n",
    "        axes[1, 1].set_ylabel('Subjectivity')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Sentiment Summary\n",
    "        summary_text = f\"\"\"Sentiment Summary:\n",
    "        \n",
    "        TextBlob:\n",
    "        â€¢ Avg Polarity: {sentiment_features['textblob_polarity'].mean():.3f}\n",
    "        â€¢ Avg Subjectivity: {sentiment_features['textblob_subjectivity'].mean():.3f}\n",
    "        \n",
    "        VADER:\n",
    "        â€¢ Avg Compound: {sentiment_features['vader_compound'].mean():.3f}\n",
    "        â€¢ Avg Positive: {sentiment_features['vader_positive'].mean():.3f}\n",
    "        â€¢ Avg Negative: {sentiment_features['vader_negative'].mean():.3f}\n",
    "        \"\"\"\n",
    "        \n",
    "        axes[1, 2].text(0.05, 0.95, summary_text, fontsize=10, transform=axes[1, 2].transAxes,\n",
    "                       verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                       facecolor=\"lightblue\", alpha=0.7))\n",
    "        axes[1, 2].set_title('Summary Statistics')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_label_distribution(self, labels, title=\"Label Distribution\"):\n",
    "        \"\"\"Plot distribution of labels/categories\"\"\"\n",
    "        if pd.isna(labels).all():\n",
    "            print(\"No label data available for plotting\")\n",
    "            return\n",
    "        \n",
    "        label_counts = pd.Series(labels).value_counts()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Bar plot\n",
    "        label_counts.plot(kind='bar', ax=axes[0], color='lightblue', edgecolor='black')\n",
    "        axes[0].set_title('Label Counts')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Pie chart\n",
    "        axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1].set_title('Label Proportions')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = DataVisualizer()\n",
    "print(\"Data visualizer initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive data visualizations\n",
    "if 'df' in locals() and not df.empty and 'original_text_data' in locals():\n",
    "    print(\"Creating Comprehensive Data Visualizations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Text Length Distribution\n",
    "    print(\"\\n1. Plotting text length distribution...\")\n",
    "    visualizer.plot_text_length_distribution(original_text_data, \"Original Text Length Distribution\")\n",
    "    \n",
    "    # 2. Word Cloud for original text\n",
    "    print(\"\\n2. Creating word cloud from original text...\")\n",
    "    visualizer.create_wordcloud(original_text_data[:200], \"Word Cloud - Original Text\")  # Use subset for performance\n",
    "    \n",
    "    # 3. Word Cloud for processed text\n",
    "    if 'text_data' in locals():\n",
    "        print(\"\\n3. Creating word cloud from processed text...\")\n",
    "        visualizer.create_wordcloud(text_data[:200], \"Word Cloud - Processed Text\")\n",
    "    \n",
    "    # 4. Label Distribution (if available)\n",
    "    if label_columns:\n",
    "        for label_col in label_columns[:2]:  # Show first 2 label columns\n",
    "            print(f\"\\n4. Plotting distribution for label: {label_col}\")\n",
    "            visualizer.plot_label_distribution(df[label_col], f\"Distribution of {label_col}\")\n",
    "    \n",
    "    # 5. Basic Features Visualization\n",
    "    if 'basic_features_df' in locals():\n",
    "        print(\"\\n5. Plotting basic feature distributions...\")\n",
    "        visualizer.plot_feature_distributions(basic_features_df, \"Basic Text Features Distribution\")\n",
    "        \n",
    "        print(\"\\n6. Creating correlation matrix for basic features...\")\n",
    "        visualizer.plot_feature_correlation_matrix(basic_features_df, \"Basic Features Correlation Matrix\")\n",
    "    \n",
    "    # 6. Advanced Features Visualization\n",
    "    if 'advanced_feature_data' in locals():\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        if 'sentiment_features' in advanced_feature_data:\n",
    "            print(\"\\n7. Creating sentiment analysis visualizations...\")\n",
    "            visualizer.plot_sentiment_analysis(advanced_feature_data['sentiment_features'], \n",
    "                                              \"Comprehensive Sentiment Analysis\")\n",
    "        \n",
    "        # Combined advanced features\n",
    "        if 'combined_features' in advanced_feature_data:\n",
    "            print(\"\\n8. Plotting advanced feature distributions...\")\n",
    "            combined_features = advanced_feature_data['combined_features']\n",
    "            \n",
    "            # Select a subset of interesting features for visualization\n",
    "            interesting_features = []\n",
    "            for col in combined_features.columns:\n",
    "                if any(keyword in col.lower() for keyword in ['polarity', 'compound', 'flesch', 'subjectivity', 'complexity']):\n",
    "                    interesting_features.append(col)\n",
    "            \n",
    "            if interesting_features:\n",
    "                subset_features = combined_features[interesting_features[:12]]\n",
    "                visualizer.plot_feature_distributions(subset_features, \n",
    "                                                    \"Key Advanced Features Distribution\")\n",
    "    \n",
    "    # 7. Feature Summary Dashboard\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE EXTRACTION SUMMARY DASHBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    feature_summary = {}\n",
    "    \n",
    "    # Basic features\n",
    "    if 'basic_features_df' in locals():\n",
    "        feature_summary['Basic Text Features'] = basic_features_df.shape[1]\n",
    "    \n",
    "    # TF-IDF features\n",
    "    if 'feature_data' in locals():\n",
    "        tfidf_features = 0\n",
    "        for key in feature_data.keys():\n",
    "            if 'tfidf' in key:\n",
    "                tfidf_features += feature_data[key].shape[1]\n",
    "        feature_summary['TF-IDF Features'] = tfidf_features\n",
    "    \n",
    "    # Advanced features\n",
    "    if 'advanced_feature_data' in locals():\n",
    "        if 'combined_features' in advanced_feature_data:\n",
    "            feature_summary['Advanced Features'] = advanced_feature_data['combined_features'].shape[1]\n",
    "    \n",
    "    # Embedding features\n",
    "    if 'embedding_feature_data' in locals():\n",
    "        embedding_features = 0\n",
    "        for feature_type, features in embedding_feature_data.items():\n",
    "            if hasattr(features, 'shape'):\n",
    "                embedding_features += features.shape[1]\n",
    "        feature_summary['Embedding Features'] = embedding_features\n",
    "    \n",
    "    # Create summary visualization\n",
    "    if feature_summary:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        categories = list(feature_summary.keys())\n",
    "        counts = list(feature_summary.values())\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "        bars = plt.bar(categories, counts, color=colors, edgecolor='black', alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
    "                    str(count), ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "        \n",
    "        plt.title('NLP Pipeline Feature Extraction Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('Feature Categories', fontsize=14)\n",
    "        plt.ylabel('Number of Features', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        total_features = sum(counts)\n",
    "        plt.text(0.02, 0.98, f'Total Features Extracted: {total_features}', \n",
    "                transform=plt.gca().transAxes, fontsize=14, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTotal features extracted across all categories: {total_features}\")\n",
    "        \n",
    "        # Feature density analysis\n",
    "        if 'df' in locals():\n",
    "            print(f\"Features per document: {total_features}\")\n",
    "            print(f\"Documents processed: {len(df)}\")\n",
    "            print(f\"Feature density: {total_features / len(df):.2f} features per document\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VISUALIZATION COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"All visualizations have been generated successfully.\")\n",
    "    print(\"The NLP pipeline has extracted and visualized comprehensive\")\n",
    "    print(\"text features for machine learning applications.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for visualization!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b2704",
   "metadata": {},
   "source": [
    "## 8. Complete NLP Pipeline Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898942e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
